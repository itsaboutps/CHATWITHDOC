Transformers enable scalable sequence modeling. Attention mechanisms improve performance.
